{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart for text retrieval part\n",
    "\n",
    "This is a notebook of quick start tutorials for text retrieval part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference\n",
    "\n",
    "We offer 2 different methods for inference. The first is use `TextEmbedder` and `TextReranker`, which only support pytorch model inference. The second is `BaseEmbedderInferenceEngine` and `BaseRerankerInferenceEngine`, which support `normal`, `onnx`, `tensorrt` inference mode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of TextEmbedder\n",
    "\n",
    "1. import TextEmbedder.\n",
    "2. load model from TextEmbedder. We support any type of dense embedding models that can be loaded by hf transformers `AutoModel.from_pretrained()`\n",
    "3. feed sentences into model and get embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Nexus import TextEmbedder\n",
    "\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "    \"The Eiffel Tower is located in Paris, France.\",\n",
    "    \"Python is a popular programming language.\",\n",
    "    \"The Great Wall of China is one of the Seven Wonders of the World.\",\n",
    "    \"Space exploration has led to many scientific discoveries.\",\n",
    "    \"Climate change is a pressing global issue.\",\n",
    "    \"The Mona Lisa is a famous painting by Leonardo da Vinci.\",\n",
    "    \"Electric cars are becoming more common.\",\n",
    "    \"The human brain is an incredibly complex organ.\"\n",
    "]\n",
    "\n",
    "\n",
    "model = TextEmbedder(model_name_or_path='/data2/OpenLLMs/bge-base-zh-v1.5', use_fp16=True, devices=['cuda:1','cuda:0']) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "embedding= model.encode(sentences, batch_size = 5)\n",
    "\n",
    "print(embedding.shape)\n",
    "print(embedding[0]@ embedding[1].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Usage of BaseEmbedderInferenceEngine\n",
    "\n",
    "1. Import AbsInferenceArguments and BaseEmbedderInferenceEngine.\n",
    "2. Get onnx model or tensorrt model. You can convert pytorch model to onnx model using class method `convert_to_onnx` or convert onnx model to tensorrt model using `convert_to_tensorrt`. If you already got onnx or tensorrt model, skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Nexus import AbsInferenceArguments, BaseEmbedderInferenceEngine\n",
    "\n",
    "model_path='/data2/OpenLLMs/bge-base-zh-v1.5'\n",
    "onnx_model_path='/data2/OpenLLMs/bge-base-zh-v1.5/onnx/model.onnx'\n",
    "\n",
    "# convert to onnx\n",
    "BaseEmbedderInferenceEngine.convert_to_onnx(model_name_or_path=model_path, onnx_model_path=onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create args, where you can specify the  param `infer_mode`. We support `normal`, `onnx`, `tensorrt` for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=AbsInferenceArguments(\n",
    "    model_name_or_path=model_path,\n",
    "    onnx_model_path=onnx_model_path,\n",
    "    trt_model_path=None,\n",
    "    infer_mode='onnx',\n",
    "    infer_device=0,\n",
    "    infer_batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Inference and get embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Inference with onnx session\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "    \"The Eiffel Tower is located in Paris, France.\",\n",
    "    \"Python is a popular programming language.\",\n",
    "    \"The Great Wall of China is one of the Seven Wonders of the World.\",\n",
    "    \"Space exploration has led to many scientific discoveries.\",\n",
    "    \"Climate change is a pressing global issue.\",\n",
    "    \"The Mona Lisa is a famous painting by Leonardo da Vinci.\",\n",
    "    \"Electric cars are becoming more common.\",\n",
    "    \"The human brain is an incredibly complex organ.\"\n",
    "]\n",
    "\n",
    "\n",
    "inference_engine_onnx = BaseEmbedderInferenceEngine(args)\n",
    "emb_onnx = inference_engine_onnx.inference(sentences, normalize=True, batch_size=5)\n",
    "print(emb_onnx.shape)\n",
    "print(emb_onnx[0]@ emb_onnx[1].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We support multi-node, multi-gpu distributed training using accelerate.\n",
    "You can provide data, training, model config files for training.\n",
    "\n",
    "Important Args:\n",
    "\n",
    "1. args for path:\n",
    "\n",
    "    1. `base_dir` : base dir of this repo\n",
    "\n",
    "    2. `train_data` : path to training data, can be files or folders. Data format should be as below:\n",
    "\n",
    "    ```json\n",
    "    {\n",
    "        \"query\": \"query\",\n",
    "        \"pos\": [\"pos 1\", \"pos 2\"],\n",
    "        \"neg\": [\"neg 1\", \"neg 2\"],\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    3. `model_name_or_path` : model name or path of base model \n",
    "\n",
    "    4. `ckpt_save_dir` : checkpoints save path\n",
    "\n",
    "    5. `deepspeed` : deepspeed config file path\n",
    "\n",
    "2. args for training:\n",
    "\n",
    "    1. `num_train_epochs` : training epochs.\n",
    "\n",
    "    2. `per_device_train_batch_size` batch size per device\n",
    "\n",
    "    3. `num_gpus` use num of gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `accelerate` to launch multi-node training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, generate accelerate config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "accelerate config --config_file accelerate_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then, launch accelerate training scripts. In multi node training, you should launch accelerate in each node in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "accelerate launch --config_file /your/accelerate/config/file /your/python/file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single node Single Device\n",
    "\n",
    "In single device mode, you should set 'negatives_cross_device' to 'false', and 'deepspeed' should be None in case it wouldn't start distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import HfArgumentParser\n",
    "\n",
    "from Nexus.training.embedder.text_retrieval import *\n",
    "import time\n",
    "def main():\n",
    "    data_config_path='/root/Nexus/examples/text_retrieval/training/embedder/data_config.json'\n",
    "    train_config_path='/root/Nexus/examples/text_retrieval/training/embedder/training_config_single_device.json'\n",
    "    model_config_path='/root/Nexus/examples/text_retrieval/training/embedder/model_config.json'\n",
    "    \n",
    "    model_args = TextEmbedderModelArguments.from_json(model_config_path)\n",
    "    data_args = TextEmbedderDataArguments.from_json(data_config_path)\n",
    "    training_args = TextEmbedderTrainingArguments.from_json(train_config_path)\n",
    "    runner = TextEmbedderRunner(\n",
    "        model_args=model_args,\n",
    "        data_args=data_args,\n",
    "        training_args=training_args\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    runner.run()\n",
    "    end = time.time()\n",
    "    elapsed_time = end-start\n",
    "    print(f\"程序运行耗时: {elapsed_time:.4f} 秒\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Node\n",
    "\n",
    "Below is python scripts for single/multi node multi device training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import HfArgumentParser\n",
    "\n",
    "from Nexus.training.embedder.text_retrieval import *\n",
    "import time\n",
    "def main():\n",
    "    data_config_path='/root/Nexus/examples/text_retrieval/training/embedder/data_config.json'\n",
    "    train_config_path='/root/Nexus/examples/text_retrieval/training/embedder/training_config.json'\n",
    "    model_config_path='/root/Nexus/examples/text_retrieval/training/embedder/model_config.json'\n",
    "    \n",
    "    model_args = TextEmbedderModelArguments.from_json(model_config_path)\n",
    "    data_args = TextEmbedderDataArguments.from_json(data_config_path)\n",
    "    training_args = TextEmbedderTrainingArguments.from_json(train_config_path)\n",
    "    runner = TextEmbedderRunner(\n",
    "        model_args=model_args,\n",
    "        data_args=data_args,\n",
    "        training_args=training_args\n",
    "    )\n",
    "    start = time.time()\n",
    "    runner.run()\n",
    "    end = time.time()\n",
    "    elapsed_time = end-start\n",
    "    print(f\"程序运行耗时: {elapsed_time:.4f} 秒\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Our evaluation step support `normal`, `onnx`, `tensorrt` inference mode for embedder and reranker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some important arguments:\n",
    "\n",
    "1. `eval_name`: your eval dataset name\n",
    "2. `dataset_dir`: your data path\n",
    "3. `splits`: default is `test`\n",
    "4. `corpus_embd_save_dir`: your corpus embedding index save path\n",
    "5. `output_dir`: eval result output dir\n",
    "6. `search_top_k`: search top k\n",
    "7. `rerank_top_k`: rerank top k\n",
    "8. `embedder_name_or_path`: embedder model path\n",
    "9. `reranker_name_or_path`: reranker model path\n",
    "10. `embedder_infer_mode`:\n",
    "    embedder infer mode, default is `None`, which means using `TextEmbedder`. Choices are `normal`, `onnx`, `tensorrt`\n",
    "\n",
    "11. `reranker_infer_mode`: same to embedder\n",
    "12. `embedder_onnx_model_path`: onnx model path, needed if you setting `embedder_infer_mode` to `onnx`\n",
    "13. `reranker_onnx_model_path`: same to embedder\n",
    "14. `embedder_trt_model_path`: tensorrt model path, needed if you setting `embedder_infer_mode` to `tensorrt`\n",
    "15. `reranker_trt_model_path`: save to embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser\n",
    "\n",
    "from Nexus.evaluation.text_retrieval.airbench import (\n",
    "    AIRBenchEvalArgs, AIRBenchEvalModelArgs,\n",
    "    AIRBenchEvalRunner\n",
    ")\n",
    "\n",
    "from Nexus.evaluation.text_retrieval.arguments import load_config\n",
    "\n",
    "def main():\n",
    "\n",
    "    eval_config_path='/root/Nexus/examples/text_retrieval/evaluation/config/eval_config.json'\n",
    "    model_config_path='/root/Nexus/examples/text_retrieval/evaluation/config/model_config.json'\n",
    "    \n",
    "    eval_args = load_config(eval_config_path, AIRBenchEvalArgs)\n",
    "    model_args = load_config(model_config_path, AIRBenchEvalModelArgs)\n",
    "    \n",
    "    eval_args: AIRBenchEvalArgs\n",
    "    model_args: AIRBenchEvalModelArgs\n",
    "\n",
    "    runner = AIRBenchEvalRunner(\n",
    "        eval_args=eval_args,\n",
    "        model_args=model_args\n",
    "    )\n",
    "\n",
    "    runner.run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"==============================================\")\n",
    "    print(\"Search results have been generated.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

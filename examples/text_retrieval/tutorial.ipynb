{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart for text retrieval part\n",
    "\n",
    "This is a notebook of quick start tutorials for text retrieval part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference\n",
    "\n",
    "We offer 2 different methods for inference. The first is use `TextEmbedder` and `TextReranker`, which only support pytorch model inference. The second is `BaseEmbedderInferenceEngine` and `BaseRerankerInferenceEngine`, which support `normal`, `onnx`, `tensorrt` inference mode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of TextEmbedder\n",
    "\n",
    "1. import TextEmbedder.\n",
    "2. load model from TextEmbedder. We support any type of dense embedding models that can be loaded by hf transformers `AutoModel.from_pretrained()`\n",
    "3. feed sentences into model and get embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InfoNexus import TextEmbedder\n",
    "\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "    \"The Eiffel Tower is located in Paris, France.\",\n",
    "    \"Python is a popular programming language.\",\n",
    "    \"The Great Wall of China is one of the Seven Wonders of the World.\",\n",
    "    \"Space exploration has led to many scientific discoveries.\",\n",
    "    \"Climate change is a pressing global issue.\",\n",
    "    \"The Mona Lisa is a famous painting by Leonardo da Vinci.\",\n",
    "    \"Electric cars are becoming more common.\",\n",
    "    \"The human brain is an incredibly complex organ.\"\n",
    "]\n",
    "\n",
    "\n",
    "model = TextEmbedder(model_name_or_path='/data2/OpenLLMs/bge-base-zh-v1.5', use_fp16=True, devices=['cuda:1','cuda:0']) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "embedding= model.encode(sentences, batch_size = 5)\n",
    "\n",
    "print(embedding.shape)\n",
    "print(embedding[0]@ embedding[1].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Usage of BaseEmbedderInferenceEngine\n",
    "\n",
    "1. Import AbsInferenceArguments and BaseEmbedderInferenceEngine.\n",
    "2. Get onnx model or tensorrt model. You can convert pytorch model to onnx model using class method `convert_to_onnx` or convert onnx model to tensorrt model using `convert_to_tensorrt`. If you already got onnx or tensorrt model, skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InfoNexus import AbsInferenceArguments, BaseEmbedderInferenceEngine\n",
    "\n",
    "model_path='/data2/OpenLLMs/bge-base-zh-v1.5'\n",
    "onnx_model_path='/data2/OpenLLMs/bge-base-zh-v1.5/onnx/model.onnx'\n",
    "\n",
    "# convert to onnx\n",
    "BaseEmbedderInferenceEngine.convert_to_onnx(model_name_or_path=model_path, onnx_model_path=onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create args, where you can specify the  param `infer_mode`. We support `normal`, `onnx`, `tensorrt` for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=AbsInferenceArguments(\n",
    "    model_name_or_path=model_path,\n",
    "    onnx_model_path=onnx_model_path,\n",
    "    trt_model_path=None,\n",
    "    infer_mode='onnx',\n",
    "    infer_device=0,\n",
    "    infer_batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Inference and get embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Inference with onnx session\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "    \"The Eiffel Tower is located in Paris, France.\",\n",
    "    \"Python is a popular programming language.\",\n",
    "    \"The Great Wall of China is one of the Seven Wonders of the World.\",\n",
    "    \"Space exploration has led to many scientific discoveries.\",\n",
    "    \"Climate change is a pressing global issue.\",\n",
    "    \"The Mona Lisa is a famous painting by Leonardo da Vinci.\",\n",
    "    \"Electric cars are becoming more common.\",\n",
    "    \"The human brain is an incredibly complex organ.\"\n",
    "]\n",
    "\n",
    "\n",
    "inference_engine_onnx = BaseEmbedderInferenceEngine(args)\n",
    "emb_onnx = inference_engine_onnx.inference(sentences, normalize=True, batch_size=5)\n",
    "print(emb_onnx.shape)\n",
    "print(emb_onnx[0]@ emb_onnx[1].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We support multi-node, multi-gpu distributed training using accelerate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single node\n",
    "\n",
    "Below is a script to launch single node multi gpus training, which is included in `examples/text_retrieval/training`\n",
    "\n",
    "Here are details about this script:\n",
    "\n",
    "1. args for path:\n",
    "\n",
    "    1. `base_dir` : base dir of this repo\n",
    "\n",
    "    2. `train_data` : path to training data, can be files or folders. Data format should be as below:\n",
    "\n",
    "    ```json\n",
    "    {\n",
    "        \"query\": \"query\",\n",
    "        \"pos\": [\"pos 1\", \"pos 2\"],\n",
    "        \"neg\": [\"neg 1\", \"neg 2\"],\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    3. `model_name_or_path` : model name or path of base model \n",
    "\n",
    "    4. `ckpt_save_dir` : checkpoints save path\n",
    "\n",
    "    5. `deepspeed` : deepspeed config file path\n",
    "\n",
    "2. args for training:\n",
    "\n",
    "    1. `num_train_epochs` : training epochs.\n",
    "\n",
    "    2. `per_device_train_batch_size` batch size per device\n",
    "\n",
    "    3. `num_gpus` use num of gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export WANDB_MODE=disabled\n",
    "\n",
    "base_dir='/data2/home/angqing/code/InfoNexus'\n",
    "train_data='/data2/home/angqing/code/InfoNexus/examples/text_retrieval/example_data/fiqa.jsonl'\n",
    "model_name_or_path='/data2/OpenLLMs/bge-base-zh-v1.5'\n",
    "ckpt_save_dir='/data2/home/angqing/code/InfoNexus/checkpoints/test_embedder'\n",
    "\n",
    "deepspeed='/data2/home/angqing/code/InfoNexus/examples/text_retrieval/training/ds_stage0.json'\n",
    "# set large epochs and small batch size for testing\n",
    "num_train_epochs=2\n",
    "per_device_train_batch_size=30\n",
    "\n",
    "# set num_gpus to 2 for testing\n",
    "num_gpus=2\n",
    "\n",
    "if [ -z \"$HF_HUB_CACHE\" ]; then\n",
    "    export HF_HUB_CACHE=\"$HOME/.cache/huggingface/hub\"\n",
    "fi\n",
    "\n",
    "model_args=\"\\\n",
    "    --model_name_or_path $model_name_or_path \\\n",
    "    --cache_dir $HF_HUB_CACHE \\\n",
    "\"\n",
    "\n",
    "data_args=\"\\\n",
    "    --train_data $train_data \\\n",
    "    --cache_path ~/.cache \\\n",
    "    --train_group_size 8 \\\n",
    "    --query_max_len 512 \\\n",
    "    --passage_max_len 512 \\\n",
    "    --pad_to_multiple_of 8 \\\n",
    "    --query_instruction_for_retrieval 'Represent this sentence for searching relevant passages: ' \\\n",
    "    --query_instruction_format '{}{}' \\\n",
    "    --knowledge_distillation False \\\n",
    "\"\n",
    "\n",
    "training_args=\"\\\n",
    "    --output_dir $ckpt_save_dir \\\n",
    "    --overwrite_output_dir \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --fp16 \\\n",
    "    --num_train_epochs $num_train_epochs \\\n",
    "    --per_device_train_batch_size $per_device_train_batch_size \\\n",
    "    --dataloader_drop_last True \\\n",
    "    --warmup_ratio 0.1 \\\n",
    "    --gradient_checkpointing \\\n",
    "    --deepspeed $deepspeed \\\n",
    "    --logging_steps 1 \\\n",
    "    --save_steps 100 \\\n",
    "    --negatives_cross_device \\\n",
    "    --temperature 0.02 \\\n",
    "    --sentence_pooling_method cls \\\n",
    "    --normalize_embeddings True \\\n",
    "    --kd_loss_type kl_div \\\n",
    "\"\n",
    "\n",
    "\n",
    "cd $base_dir\n",
    "\n",
    "cmd=\"torchrun --nproc_per_node $num_gpus \\\n",
    "    -m InfoNexus.training.embedder.text_retrieval \\\n",
    "    $model_args \\\n",
    "    $data_args \\\n",
    "    $training_args \\\n",
    "\"\n",
    "\n",
    "echo $cmd\n",
    "eval $cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi node\n",
    "\n",
    "We use `accelerate` to launch multi-node training. Scripts are in `examples/text_retrieval/training/*_accelerate.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "1. First, generate accelerate config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "accelerate config --config_file accelerate_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then, launch accelerate training scripts in each node from rank 0 to rank n.\n",
    "\n",
    "    Args are similar to single node script, except `ACCELERATE_CONFIG`, which is path to the accelerate config file generated from step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export WANDB_MODE=disabled\n",
    "\n",
    "\n",
    "BASE_DIR='/data1/home/recstudio/angqing/InfoNexus'\n",
    "\n",
    "MODEL_NAME_OR_PATH='/data1/home/recstudio/angqing/models/bge-base-zh-v1.5'\n",
    "TRAIN_DATA=\"/data1/home/recstudio/angqing/InfoNexus/eval_scripts/training/text_retrieval/example_data/fiqa.jsonl\"\n",
    "CKPT_SAVE_DIR='/data1/home/recstudio/angqing/InfoNexus/checkpoints'\n",
    "DEEPSPEED_DIR='/data1/home/recstudio/angqing/InfoNexus/eval_scripts/training/ds_stage0.json'\n",
    "ACCELERATE_CONFIG='/data1/home/recstudio/angqing/InfoNexus/eval_scripts/training/text_retrieval/accelerate_config_multi.json'\n",
    "# set large epochs and small batch size for testing\n",
    "num_train_epochs=2\n",
    "per_device_train_batch_size=16\n",
    "# set num_gpus to 2 for testing\n",
    "num_gpus=2\n",
    "\n",
    "if [ -z \"$HF_HUB_CACHE\" ]; then\n",
    "    export HF_HUB_CACHE=\"$HOME/.cache/huggingface/hub\"\n",
    "fi\n",
    "\n",
    "model_args=\"\\\n",
    "    --model_name_or_path $MODEL_NAME_OR_PATH \\\n",
    "    --cache_dir $HF_HUB_CACHE \\\n",
    "\"\n",
    "\n",
    "data_args=\"\\\n",
    "    --train_data $TRAIN_DATA \\\n",
    "    --cache_path ~/.cache \\\n",
    "    --train_group_size 8 \\\n",
    "    --query_max_len 512 \\\n",
    "    --passage_max_len 512 \\\n",
    "    --pad_to_multiple_of 8 \\\n",
    "    --query_instruction_for_retrieval 'Represent this sentence for searching relevant passages: ' \\\n",
    "    --query_instruction_format '{}{}' \\\n",
    "    --knowledge_distillation False \\\n",
    "\"\n",
    "\n",
    "training_args=\"\\\n",
    "    --output_dir $CKPT_SAVE_DIR \\\n",
    "    --overwrite_output_dir \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --fp16 \\\n",
    "    --num_train_epochs $num_train_epochs \\\n",
    "    --per_device_train_batch_size $per_device_train_batch_size \\\n",
    "    --dataloader_drop_last True \\\n",
    "    --warmup_ratio 0.1 \\\n",
    "    --gradient_checkpointing \\\n",
    "    --deepspeed $DEEPSPEED_DIR \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 500 \\\n",
    "    --negatives_cross_device \\\n",
    "    --temperature 0.02 \\\n",
    "    --sentence_pooling_method cls \\\n",
    "    --normalize_embeddings True \\\n",
    "    --kd_loss_type kl_div \\\n",
    "\"\n",
    "\n",
    "cd $BASE_DIR\n",
    "\n",
    "cmd=\"accelerate launch --config_file $ACCELERATE_CONFIG \\\n",
    "    InfoNexus/training/embedder/text_retrieval/__main__.py \\\n",
    "    $model_args \\\n",
    "    $data_args \\\n",
    "    $training_args \\\n",
    "\"\n",
    "\n",
    "echo $cmd\n",
    "eval $cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Our evaluation step support `normal`, `onnx`, `tensorrt` inference mode for embedder and reranker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Air-benchmark\n",
    "\n",
    "    We support a easy script for evaluate air-bencmark in `examples/text_retrieval/evaluation/run.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR=/data1/home/recstudio/angqing/InfoNexus\n",
    "EMBEDDER=/data1/home/recstudio/angqing/models/bge-base-zh-v1.5\n",
    "RERANKER=/data1/home/recstudio/angqing/models/bge-reranker-base\n",
    "embedder_infer_mode=onnx\n",
    "reranker_infer_mode=onnx\n",
    "embedder_onnx_path=$EMBEDDER/onnx/model.onnx\n",
    "reranker_onnx_path=$RERANKER/onnx/model.onnx\n",
    "embedder_trt_path=$EMBEDDER/trt/model.trt\n",
    "reranker_trt_path=$RERANKER/trt/model.trt\n",
    "\n",
    "\n",
    "cd $BASE_DIR\n",
    "\n",
    "\n",
    "python -m InfoNexus.evaluation.text_retrieval.airbench \\\n",
    "    --benchmark_version AIR-Bench_24.05 \\\n",
    "    --task_types qa \\\n",
    "    --domains arxiv \\\n",
    "    --languages en \\\n",
    "    --splits dev test \\\n",
    "    --output_dir $BASE_DIR/examples/text_retrieval/evaluation/air_bench/search_results \\\n",
    "    --search_top_k 1000 \\\n",
    "    --rerank_top_k 20 \\\n",
    "    --cache_dir $BASE_DIR/examples/text_retrieval/evaluation/cache/data \\\n",
    "    --overwrite False \\\n",
    "    --embedder_name_or_path $EMBEDDER \\\n",
    "    --reranker_name_or_path $RERANKER \\\n",
    "    --devices cuda:0 \\\n",
    "    --model_cache_dir $BASE_DIR/examples/text_retrieval/evaluation/cache/model \\\n",
    "    --reranker_query_max_length 128 \\\n",
    "    --reranker_max_length 512 \\\n",
    "    --embedder_infer_mode $embedder_infer_mode \\\n",
    "    --reranker_infer_mode $reranker_infer_mode \\\n",
    "    --embedder_onnx_model_path $embedder_onnx_path \\\n",
    "    --reranker_onnx_model_path $reranker_onnx_path \\\n",
    "    --embedder_trt_model_path $embedder_trt_path \\\n",
    "    --reranker_trt_model_path $reranker_trt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Custom\n",
    "\n",
    "    Below are some important arguments:\n",
    "\n",
    "    1. `eval_name`: your eval dataset name\n",
    "    2. `dataset_dir`: your data path\n",
    "    3. `splits`: default is `test`\n",
    "    4. `corpus_embd_save_dir`: your corpus embedding index save path\n",
    "    5. `output_dir`: eval result output dir\n",
    "    6. `search_top_k`: search top k\n",
    "    7. `rerank_top_k`: rerank top k\n",
    "    8. `embedder_name_or_path`: embedder model path\n",
    "    9. `reranker_name_or_path`: reranker model path\n",
    "    10. `embedder_infer_mode`:\n",
    "        embedder infer mode, default is `None`, which means using `TextEmbedder`. Choices are `normal`, `onnx`, `tensorrt`\n",
    "\n",
    "    11. `reranker_infer_mode`: same to embedder\n",
    "    12. `embedder_onnx_model_path`: onnx model path, needed if you setting `embedder_infer_mode` to `onnx`\n",
    "    13. `reranker_onnx_model_path`: same to embedder\n",
    "    14. `embedder_trt_model_path`: tensorrt model path, needed if you setting `embedder_infer_mode` to `tensorrt`\n",
    "    15. `reranker_trt_model_path`: save to embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python -m InfoNexus.evaluation.text_retrieval \\\n",
    "    --eval_name your_data_name \\\n",
    "    --dataset_dir ./your_data_path \\\n",
    "    --splits test \\\n",
    "    --corpus_embd_save_dir ./your_data_name/corpus_embd \\\n",
    "    --output_dir ./your_data_name/search_results \\\n",
    "    --search_top_k 1000 \\\n",
    "    --rerank_top_k 100 \\\n",
    "    --cache_path ./cache/data \\\n",
    "    --overwrite False \\\n",
    "    --k_values 10 100 \\\n",
    "    --eval_output_method markdown \\\n",
    "    --eval_output_path ./your_data_name/eval_results.md \\\n",
    "    --eval_metrics ndcg_at_10 recall_at_100 \\\n",
    "    --embedder_name_or_path BAAI/bge-m3 \\\n",
    "    --reranker_name_or_path BAAI/bge-reranker-v2-m3 \\\n",
    "    --devices cuda:0 cuda:1 \\\n",
    "    --cache_dir ./cache/model \\\n",
    "    --reranker_query_max_length 512 \\\n",
    "    --reranker_max_length 1024 \\\n",
    "    --embedder_infer_mode $embedder_infer_mode \\\n",
    "    --reranker_infer_mode $reranker_infer_mode \\\n",
    "    --embedder_onnx_model_path $embedder_onnx_path \\\n",
    "    --reranker_onnx_model_path $reranker_onnx_path \\\n",
    "    --embedder_trt_model_path $embedder_trt_path \\\n",
    "    --reranker_trt_model_path $reranker_trt_path"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

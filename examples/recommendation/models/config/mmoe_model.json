{
    "embedding_dim": 2,
    "combine_embeddings": true, 
    "n_experts": 3,
    "mlp_layers": [128, 128],
    "gate_layers": [32],
    "tower_layers": [32],
    "activation": "relu",
    "dropout": 0.1,
    "batch_norm": false
}
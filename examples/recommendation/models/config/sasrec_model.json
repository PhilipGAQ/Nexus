{
    "embedding_dim": 2,
    "combine_embeddings": true, 
    "num_neg": 50,
    "n_layers": 1,
    "n_heads": 2,
    "hidden_size": 64,
    "mlp_layers": [64, 8],
    "activation": "relu",
    "dropout": 0.3,
    "batch_norm": true
}